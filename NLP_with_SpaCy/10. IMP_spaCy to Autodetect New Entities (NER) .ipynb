{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Named Entity Recognition is a standard NLP task that can identify entities discussed in a text document. A Named Entity Recognizer is a model that can do this recognizing task. It should be able to identify named entities like ‘America’ , ‘Emily’ , ‘London’ ,etc.. and categorize them as PERSON, LOCATION , and so on. It is a very useful tool and helps in Information Retrival. In spacy, Named Entity Recognition is implemented by the pipeline component ner. Most of the models have it in their processing pipeline by default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T07:47:13.770394Z",
     "start_time": "2020-07-23T07:47:13.271222Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'parser', 'ner']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load a spacy model and chekc if it has ner\n",
    "# python -m spacy download en_core_web_sm\n",
    "\n",
    "import spacy\n",
    "'''\n",
    "If below doenot work, try importing en_core_web_sm first\n",
    "nlp=spacy.load('en_core_web_sm')'''\n",
    "\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In case your model does not have , you can add it using nlp.add_pipe() method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Need for Custom NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "As you saw, spaCy has in-built pipeline ner for Named recogniyion. Though it performs well, it’s not always completely accurate for your text .Sometimes , a word can be categorized as PERSON or a ORG depending upon the context. Also , sometimes the category you want may not be buit-in in spacy.\n",
    "\n",
    "Let’s have a look at how the default NER performs on an article about E-commerce companies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T07:49:49.922751Z",
     "start_time": "2020-07-23T07:49:49.582661Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "India GPE\n",
      "one CARDINAL\n",
      "Indian NORP\n",
      "USD 84 billion MONEY\n",
      "2021 DATE\n",
      "USD 24 billion MONEY\n",
      "2017 DATE\n",
      "India GPE\n",
      "Philip PERSON\n",
      "12% PERCENT\n",
      "2017 DATE\n",
      "22-25% PERCENT\n",
      "2021 DATE\n",
      "India GPE\n",
      "One CARDINAL\n",
      "Amazon ORG\n",
      "Amazon ORG\n",
      "Amazon Music Limited ORG\n",
      "2007 DATE\n",
      "Flipkart ORG\n",
      "Indian NORP\n",
      "Amazon ORG\n",
      "Walmart PERSON\n",
      "one CARDINAL\n",
      "US GPE\n",
      "Amazon ORG\n",
      "daily DATE\n",
      "2010 DATE\n",
      "Snapdeal ORG\n",
      "2011 DATE\n",
      "more than 3 CARDINAL\n",
      "India GPE\n",
      "30 million CARDINAL\n",
      "800 CARDINAL\n",
      "125,000 CARDINAL\n",
      "Indian NORP\n",
      "recent years DATE\n",
      "Indian NORP\n",
      "ShopClues PRODUCT\n",
      "July 2011 DATE\n",
      "Gurugram ORG\n",
      "INR 1.1 billion MONEY\n",
      "Nexus Venture Partners ORG\n",
      "Tiger Global PERSON\n",
      "Helion Ventures ORG\n",
      "more than 5 CARDINAL\n",
      "nine CARDINAL\n",
      "Paytm Mall PERSON\n",
      "Paytm PRODUCT\n",
      "Paytm Mall LOC\n",
      "third ORDINAL\n",
      "Reliance ORG\n",
      "Indian NORP\n",
      "Reliance ORG\n",
      "Reliance ORG\n",
      "India GPE\n",
      "two CARDINAL\n",
      "One CARDINAL\n",
      "Grofers ORG\n",
      "2013 DATE\n",
      "the last 5 years DATE\n",
      "daily DATE\n",
      "India GPE\n",
      "Indian NORP\n",
      "Asia LOC\n",
      "2020 DATE\n",
      "Digital Mall PERSON\n",
      "Asia LOC\n",
      "Yokeasia Malls PERSON\n",
      "zero CARDINAL\n",
      "monthly DATE\n",
      "one CARDINAL\n",
      "DMA ORG\n"
     ]
    }
   ],
   "source": [
    "# Performing NER on E-commerce article\n",
    "\n",
    "article_text=\"\"\"India that previously comprised only a handful of players in the e-commerce space, is now home to many biggies and giants battling out with each other to reach the top. This is thanks to the overwhelming internet and smartphone penetration coupled with the ever-increasing digital adoption across the country. These new-age innovations not only gave emerging startups a unique platform to deliver seamless shopping experiences but also provided brick and mortar stores with a level-playing field to begin their online journeys without leaving their offline legacies.\n",
    "In the wake of so many players coming together on one platform, the Indian e-commerce market is envisioned to reach USD 84 billion in 2021 from USD 24 billion in 2017. Further, with the rate at which internet penetration is increasing, we can expect more and more international retailers coming to India in addition to a large pool of new startups. This, in turn, will provide a major Philip to the organized retail market and boost its share from 12% in 2017 to 22-25% by 2021. \n",
    "Here’s a view to the e-commerce giants that are dominating India’s online shopping space:\n",
    "Amazon – One of the uncontested global leaders, Amazon started its journey as a simple online bookstore that gradually expanded its reach to provide a large suite of diversified products including media, furniture, food, and electronics, among others. And now with the launch of Amazon Prime and Amazon Music Limited, it has taken customer experience to a godly level, which will remain undefeatable for a very long time. \n",
    "Flipkart – Founded in 2007, Flipkart is recognized as the national leader in the Indian e-commerce market. Just like Amazon, it started operating by selling books and then entered other categories such as electronics, fashion, and lifestyle, mobile phones, etc. And now that it has been acquired by Walmart, one of the largest leading platforms of e-commerce in the US, it has also raised its bar of customer offerings in all aspects and giving huge competition to Amazon. \n",
    "Snapdeal – Started as a daily deals platform in 2010, Snapdeal became a full-fledged online marketplace in 2011 comprising more than 3 lac sellers across India. The platform offers over 30 million products across 800+ diverse categories from over 125,000 regional, national, and international brands and retailers. The Indian e-commerce firm follows a robust strategy to stay at the forefront of innovation and deliver seamless customer offerings to its wide customer base. It has shown great potential for recovery in recent years despite losing Freecharge and Unicommerce. \n",
    "ShopClues – Another renowned name in the Indian e-commerce industry, ShopClues was founded in July 2011. It’s a Gurugram based company having a current valuation of INR 1.1 billion and is backed by prominent names including Nexus Venture Partners, Tiger Global, and Helion Ventures as its major investors. Presently, the platform comprises more than 5 lac sellers selling products in nine different categories such as computers, cameras, mobiles, etc. \n",
    "Paytm Mall – To compete with the existing e-commerce giants, Paytm, an online payment system has also launched its online marketplace – Paytm Mall, which offers a wide array of products ranging from men and women fashion to groceries and cosmetics, electronics and home products, and many more. The unique thing about this platform is that it serves as a medium for third parties to sell their products directly through the widely-known app – Paytm. \n",
    "Reliance Retail – Given Reliance Jio’s disruptive venture in the Indian telecom space along with a solid market presence of Reliance, it is no wonder that Reliance will soon be foraying into retail space. As of now, it has plans to build an e-commerce space that will be established on online-to-offline market program and aim to bring local merchants on board to help them boost their sales and compete with the existing industry leaders. \n",
    "Big Basket – India’s biggest online supermarket, Big Basket provides a wide variety of imported and gourmet products through two types of delivery services – express delivery and slotted delivery. It also offers pre-cut fruits along with a long list of beverages including fresh juices, cold drinks, hot teas, etc. Moreover, it not only provides farm-fresh products but also ensures that the farmer gets better prices. \n",
    "Grofers – One of the leading e-commerce players in the grocery segment, Grofers started its operations in 2013 and has reached overwhelming heights in the last 5 years. Its wide range of products includes atta, milk, oil, daily need products, vegetables, dairy products, juices, beverages, among others. With its growing reach across India, it has become one of the favorite supermarkets for Indian consumers who want to shop grocery items from the comforts of their homes. \n",
    "Digital Mall of Asia – Going live in 2020, Digital Mall of Asia is a very unique concept coined by the founders of Yokeasia Malls. It is designed to provide an immersive digital space equipped with multiple visual and sensory elements to sellers and shoppers. It will also give retailers exclusive rights to sell a particular product category or brand in their respective cities. What makes it unique is its zero-commission model enabling retailers to pay only a fixed amount of monthly rental instead of paying commissions. With its one-of-a-kind features, DMA is expected to bring\n",
    "never-seen transformation to the current e-commerce ecosystem while addressing all the existing e-commerce worries such as counterfeiting. \"\"\"\n",
    "\n",
    "doc=nlp(article_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent.text,ent.label_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Observe the above output. Notice that FLIPKART has been identified as PERSON, it should have been ORG . Walmart has also been categorized wrongly as LOC , in this context it should have been ORG . Same goes for Freecharge , ShopClues ,etc..\n",
    "\n",
    "In cases like this, you’ll face the need to update and train the NER as per the context and requirements. The next section will tell you how to do it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Updating the Named Entity Recognizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In the previous section, you saw why we need to update and train the NER. Now, let’s go ahead and see how to do it.\n",
    "\n",
    "Let’s say you have variety of texts about customer statements and companies. Our task is make sure the NER recognizes the company asORGand not as PERSON , place the unidentified products under PRODUCT and so on.\n",
    "\n",
    "To enable this, you need to provide training examples which will make the NER learn for future samples.\n",
    "\n",
    "To do this, let’s use an existing pre-trained spacy model and update it with newer examples.\n",
    "\n",
    "First , let’s load a pre-existing spacy model with an in-built ner component. Then, get the Named Entity Recognizer using get_pipe() method ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T08:25:42.639404Z",
     "start_time": "2020-07-23T08:25:42.003083Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Load pre-existing spacy model\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Getting the pipeline component\n",
    "ner=nlp.get_pipe(\"ner\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "To update a pretrained model with new examples, you’ll have to provide many examples to meaningfully improve the system — a few hundred is a good start, although more is better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Format of the training examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "spaCy accepts training data as list of tuples.\n",
    "\n",
    "Each tuple should contain the text and a dictionary. The dictionary should hold the start and end indices of the named enity in the text, and the category or label of the named entity.\n",
    "\n",
    "For example, (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]})\n",
    "\n",
    "To do this, you’ll need example texts and the character offsets and labels of each entity contained in the texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T12:18:19.379878Z",
     "start_time": "2020-07-23T12:18:19.296104Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The above code clearly shows you the training format. You have to add these labels to the ner using ner.add_label() method of pipeline . Below code demonstrates the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T09:37:48.589891Z",
     "start_time": "2020-07-23T09:37:48.572932Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Adding labels to the `ner`\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Now it’s time to train the NER over these examples. But before you train, remember that apart from ner , the model has other pipeline components. These components should not get affected in training.\n",
    "\n",
    "So, disable the other pipeline components through nlp.disable_pipes() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T09:38:39.133407Z",
     "start_time": "2020-07-23T09:38:39.113459Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Disable pipeline components you dont need to change\n",
    "\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T09:46:49.003293Z",
     "start_time": "2020-07-23T09:46:48.999303Z"
    },
    "hidden": true
   },
   "source": [
    "You have to perform the training with unaffected_pipes disabled."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Training the NER model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "First, let’s understand the ideas involved before going to the code.\n",
    "* (a) To train an ner model, the model has to be looped over the example for sufficient number of iterations. \n",
    "    If you train it for like just 5 or 6 iterations, it may not be effective.\n",
    "* (b) Before every iteration it’s a good practice to shuffle the examples randomly throughrandom.shuffle() function. \n",
    "    This will ensure the model does not make generalizations based on the order of the examples.\n",
    "* (c) The training data is usually passed in batches.\n",
    "    \n",
    "    You can call the minibatch() function of spaCy over the training data that will return you data in batches . The minibatch function takes size parameter to denote the batch size. You can make use of the utility function compounding to generate an infinite series of compounding values.\n",
    "    \n",
    "    compunding() function takes three inputs which are start ( the first integer value) ,stop (the maximum value that can be generated) and finally compound. This value stored in compund is the compounding factor for the series.If you are not clear, check out this link for understanding.\n",
    "    \n",
    "    For each iteration , the model or ner is updated through the nlp.update() command. Parameters of nlp.update() are:\n",
    "    \n",
    "    * 1. docs: This expects a batch of texts as input. You can pass each batch to the zip method, which will return you batches of text and annotations. `\n",
    "\n",
    "    * 2. golds: You can pass the annotations we got through zip method here\n",
    "    * 3. drop: This represents the dropout rate.\n",
    "    * 4. losses: A dictionary to hold the losses against each pipeline component. Create an empty dictionary and pass it here.\n",
    "    \n",
    "At each word, the update() it makes a prediction. It then consults the annotations to check if the prediction is right. If it isn’t , it adjusts the weights so that the correct action will score higher next time.\n",
    "\n",
    "Finally, all of the training is done within the context of the nlp model with disabled pipeline, to prevent the other components from being involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T09:58:00.289120Z",
     "start_time": "2020-07-23T09:57:51.151272Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a new Washer\" with entities \"[(16, 22, 'PRODUCT')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a screwdriver from our neighbour\" with entities \"[(12, 22, 'PRODUCT')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I recently ordered a book from Amazon\" with entities \"[(24, 32, 'ORG')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I got my truck fixed\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I repaired my computer\" with entities \"[(15, 23, 'PRODUCT')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 2.257896077033365}\n",
      "Losses {'ner': 6.297074328555027}\n",
      "Losses {'ner': 12.20383150075213}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a tent for our trip\" with entities \"[(12, 16, 'PRODUCT')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I got my clock fixed\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I reached Chennai yesterday.\" with entities \"[(19, 28, 'GPE')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I recently ordered from Swiggy\" with entities \"[(24, 29, 'ORG')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a fancy dress\" with entities \"[(18, 23, 'PRODUCT')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I rented a camera\" with entities \"[(12, 18, 'PRODUCT')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n",
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"I bought a old table\" with entities \"[(16, 21, 'PRODUCT')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 15.319913793733576}\n",
      "Losses {'ner': 19.485396843781928}\n",
      "Losses {'ner': 0.02004672266775742}\n",
      "Losses {'ner': 1.6851258087740462}\n",
      "Losses {'ner': 5.618330500223692}\n",
      "Losses {'ner': 9.215008034494986}\n",
      "Losses {'ner': 10.892650872922346}\n",
      "Losses {'ner': 1.9075554603408875}\n",
      "Losses {'ner': 2.7656420005626217}\n",
      "Losses {'ner': 6.6573935141849745}\n",
      "Losses {'ner': 9.62119882027173}\n",
      "Losses {'ner': 11.507276407801328}\n",
      "Losses {'ner': 3.233238219057057}\n",
      "Losses {'ner': 6.756251601268559}\n",
      "Losses {'ner': 13.724239178462994}\n",
      "Losses {'ner': 13.732705066376866}\n",
      "Losses {'ner': 13.7502912575153}\n",
      "Losses {'ner': 1.3111458399555431}\n",
      "Losses {'ner': 3.211543275074064}\n",
      "Losses {'ner': 6.328140176443185}\n",
      "Losses {'ner': 10.16847120063403}\n",
      "Losses {'ner': 11.287935155444302}\n",
      "Losses {'ner': 0.04344710907200522}\n",
      "Losses {'ner': 2.555033088955426}\n",
      "Losses {'ner': 3.930146811125198}\n",
      "Losses {'ner': 4.1849705441015175}\n",
      "Losses {'ner': 5.404631370812851}\n",
      "Losses {'ner': 3.2492175296466854}\n",
      "Losses {'ner': 7.740058992854884}\n",
      "Losses {'ner': 7.834590409456851}\n",
      "Losses {'ner': 9.879728346084576}\n",
      "Losses {'ner': 9.881453474210275}\n",
      "Losses {'ner': 1.1210672553861514}\n",
      "Losses {'ner': 4.472170332115468}\n",
      "Losses {'ner': 4.481672857016122}\n",
      "Losses {'ner': 5.255230145802955}\n",
      "Losses {'ner': 5.372415656924261}\n",
      "Losses {'ner': 1.2426016374390372}\n",
      "Losses {'ner': 3.832547480618059}\n",
      "Losses {'ner': 4.520096281621006}\n",
      "Losses {'ner': 7.800508203623719}\n",
      "Losses {'ner': 7.800511282021121}\n",
      "Losses {'ner': 6.0951899864214454e-05}\n",
      "Losses {'ner': 1.7543591468753545}\n",
      "Losses {'ner': 5.71613492126665}\n",
      "Losses {'ner': 7.170353359049478}\n",
      "Losses {'ner': 7.180028540553807}\n",
      "Losses {'ner': 0.7866699529522521}\n",
      "Losses {'ner': 2.3475487155055816}\n",
      "Losses {'ner': 5.9910286775357235}\n",
      "Losses {'ner': 11.179859655583641}\n",
      "Losses {'ner': 11.30695110603644}\n",
      "Losses {'ner': 1.6737008353339946}\n",
      "Losses {'ner': 1.8574341443039715}\n",
      "Losses {'ner': 1.9457131495029123}\n",
      "Losses {'ner': 3.661126025847968}\n",
      "Losses {'ner': 3.661126962588892}\n",
      "Losses {'ner': 0.0008873125271051663}\n",
      "Losses {'ner': 0.5290808975280754}\n",
      "Losses {'ner': 1.8465733618179918}\n",
      "Losses {'ner': 3.1863896055142398}\n",
      "Losses {'ner': 4.647766952659097}\n",
      "Losses {'ner': 0.9741195629842423}\n",
      "Losses {'ner': 2.5153073442177285}\n",
      "Losses {'ner': 3.9685478989754954}\n",
      "Losses {'ner': 4.472170070418876}\n",
      "Losses {'ner': 6.403767920063091}\n",
      "Losses {'ner': 0.0019109526823126721}\n",
      "Losses {'ner': 0.11363453204499468}\n",
      "Losses {'ner': 0.11483259508621008}\n",
      "Losses {'ner': 0.4714697230956936}\n",
      "Losses {'ner': 0.6170965259437786}\n",
      "Losses {'ner': 1.9129761646304928}\n",
      "Losses {'ner': 3.322865121630798}\n",
      "Losses {'ner': 3.327437307259814}\n",
      "Losses {'ner': 5.517522883580227}\n",
      "Losses {'ner': 5.531549778757838}\n",
      "Losses {'ner': 0.4354389485783843}\n",
      "Losses {'ner': 0.8426422135977123}\n",
      "Losses {'ner': 0.9709842639118189}\n",
      "Losses {'ner': 2.233582676946658}\n",
      "Losses {'ner': 2.2347470784770938}\n",
      "Losses {'ner': 0.02961697810019359}\n",
      "Losses {'ner': 0.09531749437553572}\n",
      "Losses {'ner': 1.100244401443092}\n",
      "Losses {'ner': 1.9060316783694824}\n",
      "Losses {'ner': 1.9068985262144955}\n",
      "Losses {'ner': 0.34136370489317436}\n",
      "Losses {'ner': 0.34879796783990535}\n",
      "Losses {'ner': 1.4774282674626051}\n",
      "Losses {'ner': 1.5662832133052873}\n",
      "Losses {'ner': 1.5663885831016957}\n",
      "Losses {'ner': 3.2112401526114466e-09}\n",
      "Losses {'ner': 0.0344308934658235}\n",
      "Losses {'ner': 0.08254374425744881}\n",
      "Losses {'ner': 1.245516909662025}\n",
      "Losses {'ner': 1.2465546883182679}\n",
      "Losses {'ner': 1.3125108611967192e-06}\n",
      "Losses {'ner': 0.0008247431232697789}\n",
      "Losses {'ner': 0.0032866222511552037}\n",
      "Losses {'ner': 0.003286933437494964}\n",
      "Losses {'ner': 0.12100896688203915}\n",
      "Losses {'ner': 0.000502492585403802}\n",
      "Losses {'ner': 0.001599642371360049}\n",
      "Losses {'ner': 0.0016051878629030584}\n",
      "Losses {'ner': 0.03299401513951121}\n",
      "Losses {'ner': 0.29255948129441306}\n",
      "Losses {'ner': 6.296827361713248e-05}\n",
      "Losses {'ner': 0.00015389691222595508}\n",
      "Losses {'ner': 0.10975523847624316}\n",
      "Losses {'ner': 0.10975767963343372}\n",
      "Losses {'ner': 2.1087022059636977}\n",
      "Losses {'ner': 0.01670850115653925}\n",
      "Losses {'ner': 0.016708718900313925}\n",
      "Losses {'ner': 0.03951746941155603}\n",
      "Losses {'ner': 0.039529402531048634}\n",
      "Losses {'ner': 0.0421765727851151}\n",
      "Losses {'ner': 5.30571734859511e-05}\n",
      "Losses {'ner': 0.0009609712597550672}\n",
      "Losses {'ner': 0.10212376051728707}\n",
      "Losses {'ner': 0.10212943485774073}\n",
      "Losses {'ner': 0.1451340912305655}\n",
      "Losses {'ner': 0.026002297106064276}\n",
      "Losses {'ner': 2.0176040504641084}\n",
      "Losses {'ner': 2.0192529670952957}\n",
      "Losses {'ner': 2.019252980774175}\n",
      "Losses {'ner': 2.020606902923131}\n",
      "Losses {'ner': 1.1159516707283174e-05}\n",
      "Losses {'ner': 0.022536349921763812}\n",
      "Losses {'ner': 0.023343811873342026}\n",
      "Losses {'ner': 0.03701465403332085}\n",
      "Losses {'ner': 0.037016411448673014}\n",
      "Losses {'ner': 0.001283380237899529}\n",
      "Losses {'ner': 0.005137924554197651}\n",
      "Losses {'ner': 0.00653126882219904}\n",
      "Losses {'ner': 0.006531499035709587}\n",
      "Losses {'ner': 0.006531904700609259}\n",
      "Losses {'ner': 4.892131846290226e-06}\n",
      "Losses {'ner': 4.8941075404401735e-06}\n",
      "Losses {'ner': 0.0018742557378102903}\n",
      "Losses {'ner': 0.001881346207304503}\n",
      "Losses {'ner': 0.0019080574390833202}\n",
      "Losses {'ner': 0.019669744705515836}\n",
      "Losses {'ner': 0.019726302976939263}\n",
      "Losses {'ner': 0.01973192379796875}\n",
      "Losses {'ner': 2.0950566887352045}\n",
      "Losses {'ner': 2.109921926027159}\n"
     ]
    }
   ],
   "source": [
    "# Import requirements\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "\n",
    "# TRAINING THE MODEL\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "\n",
    "  # Training for 30 iterations\n",
    "  for iteration in range(30):\n",
    "\n",
    "    # shuufling examples  before every iteration\n",
    "    random.shuffle(TRAIN_DATA)\n",
    "    losses = {}\n",
    "    # batch up the examples using spaCy's minibatch\n",
    "    batches = minibatch(TRAIN_DATA, size=compounding(4.0, 32.0, 1.001))\n",
    "    for batch in batches:\n",
    "        texts, annotations = zip(*batch)\n",
    "        nlp.update(\n",
    "                    texts,  # batch of texts\n",
    "                    annotations,  # batch of annotations\n",
    "                    drop=0.5,  # dropout - make it harder to memorise data\n",
    "                    losses=losses,\n",
    "                )\n",
    "        print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Let’s predict on new texts the model has not seen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Training of our NER is complete now. You can test if the ner is now working as you expected. If it’s not up to your expectations, include more training examples and try again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T09:59:36.731685Z",
     "start_time": "2020-07-23T09:59:36.714225Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities [('Swift', 'PRODUCT')]\n"
     ]
    }
   ],
   "source": [
    "# Testing the model\n",
    "doc = nlp(\"I was driving a Swift\")\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "You can observe that even though I didn’t directly train the model to recognize “Alto” as a vehicle name, it has predicted based on the similarity of context.\n",
    "\n",
    "This is the awesome part of the NER model.\n",
    "\n",
    "The model does not just memorize the training examples. It should learn from them and be able to generalize it to new examples.\n",
    "Once you find the performance of the model satisfactory, save the updated model.\n",
    "\n",
    "You can save it your desired directory through the to_disk command.\n",
    "\n",
    "After saving, you can load the model from the directory at any point of time by passing the directory path to spacy.load() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T10:00:34.095746Z",
     "start_time": "2020-07-23T10:00:33.075837Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to \\content\n",
      "Loading from \\content\n",
      "Entities [('Fridge', 'PRODUCT'), ('FlipKart', 'ORG')]\n"
     ]
    }
   ],
   "source": [
    "# Save the  model to directory\n",
    "output_dir = Path('/content/')\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Load the saved model and predict\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp_updated = spacy.load(output_dir)\n",
    "doc = nlp_updated(\"Fridge can be ordered in FlipKart\" )\n",
    "print(\"Entities\", [(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The above output shows that our model has been updated and works as per our expectations. This is how you can update and train the Named Entity Recognizer of any existing model in spaCy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## How to train NER from a blank SpaCy model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "If you don’t want to use a pre-existing model, you can create an empty model using spacy.blank() by just passing the language ID. For creating an empty model in the English language, you have to pass “en”.\n",
    "\n",
    "After this, most of the steps for training the NER are similar. The key points to remember are:\n",
    "* 1. As it is an empty model , it does not have any pipeline component by default. You have to add the ner to the pipeline through add_pipe() method.\n",
    "* 2. You’ll not have to disable other pipelines as in previous case\n",
    "* 3. You must provide a larger number of training examples comparitively in this case.\n",
    "* 4. Before you start training the new model set nlp.begin_training().\n",
    "\n",
    "The below code shows the initial steps for training NER of a new empty model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T12:35:30.311070Z",
     "start_time": "2020-07-23T12:35:29.720646Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<thinc.neural.optimizers.Optimizer at 0x200160e6a58>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train NER from a blank spacy model\n",
    "import spacy\n",
    "\n",
    "nlp=spacy.blank(\"en\")\n",
    "nlp.add_pipe(nlp.create_pipe('ner'))\n",
    "nlp.begin_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "After this, you can follow the same exact procedure as in the case for pre-existing model.\n",
    "\n",
    "This is how you can train the named entity recognizer to identify and categorize correctly as per the context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training completely new entity type in spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In previous section, we saw how to train the ner to categorize correctly.\n",
    "\n",
    "What if you want to place an entity in a category that’s not already present?\n",
    "\n",
    "Consider you have a lot of text data on the food consumed in diverse areas. And you want the NER to classify all the food items under the category FOOD. But, there’s no such existing category.\n",
    "\n",
    "What can you do ?\n",
    "\n",
    "spaCy is highly flexible and allows you to add a new entity type and train the model. This feature is extremely useful as it allows you to add new entity types for easier information retrieval. This section explains how to implement it.\n",
    "\n",
    "First , load the pre-existing spacy model you want to use and get the ner pipeline throughget_pipe() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T09:06:13.329043Z",
     "start_time": "2020-07-24T09:06:06.817842Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_core_web_sm' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Import and load the spacy model\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Getting the ner component\n",
    "ner=nlp.get_pipe('ner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, store the name of new category / entity type in a string variable LABEL.\n",
    "\n",
    "Now, how will the model know which entities to be classified under the new label ?\n",
    "\n",
    "You will have to train the model with examples. The training examples should teach the model what type of entities should be classified as FOOD.\n",
    "\n",
    "Let us prepare the training data.\n",
    "\n",
    "The format of the training data is a list of tuples. Each tuple contains the example text and a dictionary. The dictionary will have the key entities , that stores the start and end indices along with the label of the entitties present in the text.\n",
    "\n",
    "For example , To pass “Pizza is a common fast food” as example the format will be : (\"Pizza is a common fast food\",{\"entities\" : [(0, 5, \"FOOD\")]})\n",
    "\n",
    "The below code shows the training data I have prepared."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T09:06:16.329664Z",
     "start_time": "2020-07-24T09:06:16.322683Z"
    }
   },
   "outputs": [],
   "source": [
    "# New label to add\n",
    "LABEL = \"FOOD\"\n",
    "\n",
    "# Training examples in the required format\n",
    "TRAIN_DATA =[ (\"Pizza is a common fast food.\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"Pasta is an italian recipe\", {\"entities\": [(0, 5, \"FOOD\")]}),\n",
    "              (\"China's noodles are very famous\", {\"entities\": [(8,14, \"FOOD\")]}),\n",
    "              (\"Shrimps are famous in China too\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Lasagna is another classic of Italy\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Sushi is extemely famous and expensive Japanese dish\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Unagi is a famous seafood of Japan\", {\"entities\": [(0,5, \"FOOD\")]}),\n",
    "              (\"Tempura , Soba are other famous dishes of Japan\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Udon is a healthy type of noodles\", {\"entities\": [(0,4, \"ORG\")]}),\n",
    "              (\"Chocolate soufflé is extremely famous french cuisine\", {\"entities\": [(0,17, \"FOOD\")]}),\n",
    "              (\"Flamiche is french pastry\", {\"entities\": [(0,8, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Burgers are the most commonly consumed fastfood\", {\"entities\": [(0,7, \"FOOD\")]}),\n",
    "              (\"Frenchfries are considered too oily\", {\"entities\": [(0,11, \"FOOD\")]})\n",
    "           ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the training data is ready, we can go ahead to see how these examples are used to train the ner.\n",
    "\n",
    "Remember the label “FOOD” label is not known to the model now.\n",
    "\n",
    "So, our first task will be to add the label to ner through add_label() method. Next, you can use resume_training() function to return an optimizer.\n",
    "\n",
    "Also , when training is done the other pipeline components will also get affected. To prevent these, use disable_pipes() method to disable all other pipes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T09:06:20.973017Z",
     "start_time": "2020-07-24T09:06:19.823481Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add the new label to ner\n",
    "ner.add_label(LABEL)\n",
    "\n",
    "# Resume training\n",
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "\n",
    "# List of pipes you want to train\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "\n",
    "# List of pipes which should remain unaffected in training\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few key things to take care of:\n",
    "\n",
    "* a) You have to pass the examples through the model for a sufficient number of iterations. Here, I implement 30 iterations.\n",
    "* b) Remember to fine-tune the model of iterations according to performance. Also, before every iteration it’s better to shuffle the examples randomly throughrandom.shuffle() function . This will ensure the model does not make generalizations based on the order of the examples.\n",
    "* c) The training data has to be passed in batches. You can call the minibatch() function of spaCy over the training examples that will return you data in batches . A parameter of minibatch function is size, denoting the batch size.\n",
    "\n",
    "For each iteration , the model or ner is update through the nlp.update() command. Parameters of nlp.update() are :\n",
    "\n",
    "* docs : This expects a batch of texts as input. You can pass each batch to the zip method , which will return you batches of text and annotations.\n",
    "* sgd : You have to pass the optimizer that was returned by resume_training() here.\n",
    "* golds : You can pass the annotations we got through zip method here\n",
    "* drop : This represents the dropout rate.\n",
    "* losses: A dictionary to hold the losses against each pipeline component. Create an empty dictionary and pass it here.\n",
    "\n",
    "At each word,the update() it makes a prediction. It then consults the annotations to check if the prediction is right. If it isn’t, it adjusts the weights so that the correct action will score higher next time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T09:06:44.130129Z",
     "start_time": "2020-07-24T09:06:24.004310Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\language.py:482: UserWarning: [W030] Some entities could not be aligned in the text \"China's noodles are very famous\" with entities \"[(8, 14, 'FOOD')]\". Use `spacy.gold.biluo_tags_from_offsets(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  gold = GoldParse(doc, **gold)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 4.749333207933423}\n",
      "Losses {'ner': 8.781804432071558}\n",
      "Losses {'ner': 12.508620587606641}\n",
      "Losses {'ner': 15.47218810654722}\n",
      "Losses {'ner': 21.186205070885926}\n",
      "Losses {'ner': 25.520295780595138}\n",
      "Losses {'ner': 30.175837983335605}\n",
      "Losses {'ner': 35.06680448486646}\n",
      "Losses {'ner': 39.45858201990632}\n",
      "Losses {'ner': 43.43084456251382}\n",
      "Losses {'ner': 45.397397705322355}\n",
      "Losses {'ner': 52.23514240214222}\n",
      "Losses {'ner': 61.555120670842875}\n",
      "Losses {'ner': 65.56715345006643}\n",
      "Losses {'ner': 5.416058388203396}\n",
      "Losses {'ner': 12.062112715691342}\n",
      "Losses {'ner': 17.461527023774508}\n",
      "Losses {'ner': 20.759330655234763}\n",
      "Losses {'ner': 29.44021169168026}\n",
      "Losses {'ner': 38.12220996582927}\n",
      "Losses {'ner': 41.46708919180254}\n",
      "Losses {'ner': 46.129301859789294}\n",
      "Losses {'ner': 53.23371006052493}\n",
      "Losses {'ner': 54.98497648178399}\n",
      "Losses {'ner': 58.19075001918044}\n",
      "Losses {'ner': 63.4953347505283}\n",
      "Losses {'ner': 69.84568238865849}\n",
      "Losses {'ner': 72.04268160534356}\n",
      "Losses {'ner': 1.9821537178322615}\n",
      "Losses {'ner': 5.210098617313633}\n",
      "Losses {'ner': 17.149215095279942}\n",
      "Losses {'ner': 21.860681824920903}\n",
      "Losses {'ner': 28.680791797534766}\n",
      "Losses {'ner': 38.276152523652854}\n",
      "Losses {'ner': 44.92034095753843}\n",
      "Losses {'ner': 46.23708346204148}\n",
      "Losses {'ner': 49.35397035707501}\n",
      "Losses {'ner': 54.87183529962567}\n",
      "Losses {'ner': 58.01008325894145}\n",
      "Losses {'ner': 66.12472371358672}\n",
      "Losses {'ner': 69.27754609563635}\n",
      "Losses {'ner': 77.27563612439917}\n",
      "Losses {'ner': 8.945581961423159}\n",
      "Losses {'ner': 11.232596889138222}\n",
      "Losses {'ner': 18.300863813608885}\n",
      "Losses {'ner': 23.04362688586116}\n",
      "Losses {'ner': 28.657284762710333}\n",
      "Losses {'ner': 32.87783374264836}\n",
      "Losses {'ner': 34.47262955154292}\n",
      "Losses {'ner': 34.931145269656554}\n",
      "Losses {'ner': 41.03301240433939}\n",
      "Losses {'ner': 46.21294424828375}\n",
      "Losses {'ner': 52.34636129491264}\n",
      "Losses {'ner': 57.22185343556339}\n",
      "Losses {'ner': 58.526594020368066}\n",
      "Losses {'ner': 61.525086455920245}\n",
      "Losses {'ner': 2.346924841287546}\n",
      "Losses {'ner': 9.81302329886239}\n",
      "Losses {'ner': 11.274907422135584}\n",
      "Losses {'ner': 16.950984708731994}\n",
      "Losses {'ner': 21.286213754152413}\n",
      "Losses {'ner': 23.744036801799666}\n",
      "Losses {'ner': 25.951215120672714}\n",
      "Losses {'ner': 31.664428610762116}\n",
      "Losses {'ner': 34.12010873464169}\n",
      "Losses {'ner': 38.39750510553131}\n",
      "Losses {'ner': 40.46128000377212}\n",
      "Losses {'ner': 44.446538542048074}\n",
      "Losses {'ner': 47.595113381627016}\n",
      "Losses {'ner': 51.89822434063535}\n",
      "Losses {'ner': 1.6157890800386667}\n",
      "Losses {'ner': 7.356790344230831}\n",
      "Losses {'ner': 12.751173594821012}\n",
      "Losses {'ner': 15.312226108071627}\n",
      "Losses {'ner': 22.107249445078196}\n",
      "Losses {'ner': 22.216851062275964}\n",
      "Losses {'ner': 25.578913880181062}\n",
      "Losses {'ner': 27.801079005952488}\n",
      "Losses {'ner': 30.404174885497923}\n",
      "Losses {'ner': 33.54852393657711}\n",
      "Losses {'ner': 39.68427649901059}\n",
      "Losses {'ner': 41.63745377642044}\n",
      "Losses {'ner': 47.426074349587}\n",
      "Losses {'ner': 52.59737619132284}\n",
      "Losses {'ner': 2.0887981520500034}\n",
      "Losses {'ner': 7.470606110757217}\n",
      "Losses {'ner': 11.917756287613884}\n",
      "Losses {'ner': 14.247328177181771}\n",
      "Losses {'ner': 15.246821963897673}\n",
      "Losses {'ner': 21.3267525829433}\n",
      "Losses {'ner': 22.681936263990792}\n",
      "Losses {'ner': 23.83798025473152}\n",
      "Losses {'ner': 30.733732622906246}\n",
      "Losses {'ner': 34.14700599966818}\n",
      "Losses {'ner': 37.59754699064433}\n",
      "Losses {'ner': 44.25234138561427}\n",
      "Losses {'ner': 49.784177490906586}\n",
      "Losses {'ner': 52.493612117530574}\n",
      "Losses {'ner': 1.003165043599438}\n",
      "Losses {'ner': 5.342674849991454}\n",
      "Losses {'ner': 6.312476553262968}\n",
      "Losses {'ner': 11.296403437299887}\n",
      "Losses {'ner': 16.571371167345205}\n",
      "Losses {'ner': 24.00667207260267}\n",
      "Losses {'ner': 24.982219571073074}\n",
      "Losses {'ner': 24.98627967151333}\n",
      "Losses {'ner': 28.750341558330547}\n",
      "Losses {'ner': 35.685254166801315}\n",
      "Losses {'ner': 38.67786652508585}\n",
      "Losses {'ner': 41.64540946410337}\n",
      "Losses {'ner': 43.63004760076213}\n",
      "Losses {'ner': 49.45726338592279}\n",
      "Losses {'ner': 2.048848502814508}\n",
      "Losses {'ner': 7.31146799331691}\n",
      "Losses {'ner': 10.34145596896633}\n",
      "Losses {'ner': 13.060826240904134}\n",
      "Losses {'ner': 16.039910788208545}\n",
      "Losses {'ner': 17.373246270258278}\n",
      "Losses {'ner': 22.20977580084309}\n",
      "Losses {'ner': 25.152897618022507}\n",
      "Losses {'ner': 28.010548262051998}\n",
      "Losses {'ner': 32.59357662411642}\n",
      "Losses {'ner': 39.322102515907346}\n",
      "Losses {'ner': 44.5443985403472}\n",
      "Losses {'ner': 50.55227685627327}\n",
      "Losses {'ner': 51.51414217904497}\n",
      "Losses {'ner': 5.408998284488916}\n",
      "Losses {'ner': 13.890494380146265}\n",
      "Losses {'ner': 22.41580514679663}\n",
      "Losses {'ner': 26.06953207720983}\n",
      "Losses {'ner': 30.194148385347603}\n",
      "Losses {'ner': 33.82674261220711}\n",
      "Losses {'ner': 42.10024659571684}\n",
      "Losses {'ner': 45.08718451627976}\n",
      "Losses {'ner': 52.472237433572104}\n",
      "Losses {'ner': 54.76848835373278}\n",
      "Losses {'ner': 55.73386500322488}\n",
      "Losses {'ner': 58.55987146300106}\n",
      "Losses {'ner': 63.39732659225194}\n",
      "Losses {'ner': 64.38018982404685}\n",
      "Losses {'ner': 3.148770060040988}\n",
      "Losses {'ner': 7.096933312364854}\n",
      "Losses {'ner': 9.393534266157076}\n",
      "Losses {'ner': 15.96619720524177}\n",
      "Losses {'ner': 23.060774718876928}\n",
      "Losses {'ner': 26.32558817230165}\n",
      "Losses {'ner': 29.797221129760146}\n",
      "Losses {'ner': 35.14456779975444}\n",
      "Losses {'ner': 40.197254904542206}\n",
      "Losses {'ner': 46.523384986288875}\n",
      "Losses {'ner': 51.5893487433259}\n",
      "Losses {'ner': 55.144801923603154}\n",
      "Losses {'ner': 59.81961131544631}\n",
      "Losses {'ner': 59.88696757221442}\n",
      "Losses {'ner': 4.142498156779993}\n",
      "Losses {'ner': 11.04310055708288}\n",
      "Losses {'ner': 13.125494088537607}\n",
      "Losses {'ner': 16.531980307332788}\n",
      "Losses {'ner': 22.00738702111653}\n",
      "Losses {'ner': 26.832487311327213}\n",
      "Losses {'ner': 30.928157152369238}\n",
      "Losses {'ner': 35.25349648133255}\n",
      "Losses {'ner': 37.963522817764385}\n",
      "Losses {'ner': 40.79779449068832}\n",
      "Losses {'ner': 43.344719271044085}\n",
      "Losses {'ner': 48.72891627145822}\n",
      "Losses {'ner': 52.580528407226154}\n",
      "Losses {'ner': 58.87728851442938}\n",
      "Losses {'ner': 5.848689004778862}\n",
      "Losses {'ner': 9.917507800098974}\n",
      "Losses {'ner': 12.688541307870764}\n",
      "Losses {'ner': 18.151823893014807}\n",
      "Losses {'ner': 23.79280263261171}\n",
      "Losses {'ner': 28.97002564446302}\n",
      "Losses {'ner': 35.79610000626417}\n",
      "Losses {'ner': 42.128401386376936}\n",
      "Losses {'ner': 45.54547273629578}\n",
      "Losses {'ner': 49.48588048183592}\n",
      "Losses {'ner': 53.14363147802942}\n",
      "Losses {'ner': 60.507201768647064}\n",
      "Losses {'ner': 61.47262376565777}\n",
      "Losses {'ner': 66.17463763729029}\n",
      "Losses {'ner': 2.2015520269051194}\n",
      "Losses {'ner': 5.7192661399021745}\n",
      "Losses {'ner': 6.9153045321581885}\n",
      "Losses {'ner': 12.935392713057809}\n",
      "Losses {'ner': 17.628888977924362}\n",
      "Losses {'ner': 21.595037719467655}\n",
      "Losses {'ner': 28.974391943076625}\n",
      "Losses {'ner': 32.68050680365661}\n",
      "Losses {'ner': 38.37501500701285}\n",
      "Losses {'ner': 41.20510413326076}\n",
      "Losses {'ner': 43.672765225318585}\n",
      "Losses {'ner': 49.536429488945146}\n",
      "Losses {'ner': 52.25109496036123}\n",
      "Losses {'ner': 55.545805845861764}\n",
      "Losses {'ner': 2.4477605731226504}\n",
      "Losses {'ner': 6.170499910658691}\n",
      "Losses {'ner': 11.820017268008087}\n",
      "Losses {'ner': 18.001221494807396}\n",
      "Losses {'ner': 22.84153050419991}\n",
      "Losses {'ner': 29.508547360572265}\n",
      "Losses {'ner': 29.543464161117754}\n",
      "Losses {'ner': 34.978955126960955}\n",
      "Losses {'ner': 39.31854033663376}\n",
      "Losses {'ner': 40.517773081101495}\n",
      "Losses {'ner': 46.45917624627009}\n",
      "Losses {'ner': 51.371628723519734}\n",
      "Losses {'ner': 54.18848433852463}\n",
      "Losses {'ner': 59.35830236733227}\n",
      "Losses {'ner': 2.3196188170913956}\n",
      "Losses {'ner': 4.602070271015691}\n",
      "Losses {'ner': 7.211768775754081}\n",
      "Losses {'ner': 9.70865421001281}\n",
      "Losses {'ner': 13.791658477542569}\n",
      "Losses {'ner': 18.70347363377141}\n",
      "Losses {'ner': 20.41917363755556}\n",
      "Losses {'ner': 28.206210169626047}\n",
      "Losses {'ner': 28.250360654486087}\n",
      "Losses {'ner': 35.12765696067896}\n",
      "Losses {'ner': 41.14330345001605}\n",
      "Losses {'ner': 47.62351013905669}\n",
      "Losses {'ner': 53.28934292669243}\n",
      "Losses {'ner': 59.00384287710136}\n",
      "Losses {'ner': 3.557426513398241}\n",
      "Losses {'ner': 4.593410634632164}\n",
      "Losses {'ner': 5.579305898740131}\n",
      "Losses {'ner': 7.5547856066332315}\n",
      "Losses {'ner': 10.4078462401053}\n",
      "Losses {'ner': 10.409057694119163}\n",
      "Losses {'ner': 17.472393388491817}\n",
      "Losses {'ner': 20.358142231662136}\n",
      "Losses {'ner': 25.896624205705088}\n",
      "Losses {'ner': 31.633841808199726}\n",
      "Losses {'ner': 36.379437656794984}\n",
      "Losses {'ner': 39.2342614866922}\n",
      "Losses {'ner': 45.0186516381929}\n",
      "Losses {'ner': 49.36598205067997}\n",
      "Losses {'ner': 2.603451821487397}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3.5856970660897787}\n",
      "Losses {'ner': 6.498417640950493}\n",
      "Losses {'ner': 11.228415901809967}\n",
      "Losses {'ner': 16.331599679790997}\n",
      "Losses {'ner': 22.098773113583775}\n",
      "Losses {'ner': 26.900591873346457}\n",
      "Losses {'ner': 32.095414959608206}\n",
      "Losses {'ner': 33.630987088234825}\n",
      "Losses {'ner': 41.12813890077333}\n",
      "Losses {'ner': 46.43831359236432}\n",
      "Losses {'ner': 50.22174381106091}\n",
      "Losses {'ner': 55.27535620782922}\n",
      "Losses {'ner': 58.735605179681926}\n",
      "Losses {'ner': 0.9966341674064552}\n",
      "Losses {'ner': 4.143451926971011}\n",
      "Losses {'ner': 9.17846764403248}\n",
      "Losses {'ner': 12.696337010298066}\n",
      "Losses {'ner': 14.645755287663178}\n",
      "Losses {'ner': 16.09604432611968}\n",
      "Losses {'ner': 19.609707510017415}\n",
      "Losses {'ner': 23.492205730909063}\n",
      "Losses {'ner': 26.168501783689408}\n",
      "Losses {'ner': 28.456475559043156}\n",
      "Losses {'ner': 29.48034508250771}\n",
      "Losses {'ner': 32.42303361760423}\n",
      "Losses {'ner': 37.45543896449817}\n",
      "Losses {'ner': 38.902148840417134}\n",
      "Losses {'ner': 3.3540508627447707}\n",
      "Losses {'ner': 5.882194267902378}\n",
      "Losses {'ner': 8.42190753194427}\n",
      "Losses {'ner': 11.064639548929478}\n",
      "Losses {'ner': 16.37024745756767}\n",
      "Losses {'ner': 16.370256434321192}\n",
      "Losses {'ner': 22.12025837166314}\n",
      "Losses {'ner': 28.478678167183745}\n",
      "Losses {'ner': 29.77573481174207}\n",
      "Losses {'ner': 35.69712052823982}\n",
      "Losses {'ner': 37.551509509266026}\n",
      "Losses {'ner': 42.5182513403079}\n",
      "Losses {'ner': 45.79601187210245}\n",
      "Losses {'ner': 50.92252968816201}\n",
      "Losses {'ner': 8.487744432874024}\n",
      "Losses {'ner': 15.324227961711586}\n",
      "Losses {'ner': 18.451590072421823}\n",
      "Losses {'ner': 23.97748762090808}\n",
      "Losses {'ner': 27.086072577735173}\n",
      "Losses {'ner': 29.956876988914225}\n",
      "Losses {'ner': 29.959663189724303}\n",
      "Losses {'ner': 38.59726325400195}\n",
      "Losses {'ner': 40.529626348357795}\n",
      "Losses {'ner': 42.595946167508316}\n",
      "Losses {'ner': 51.3769114184833}\n",
      "Losses {'ner': 55.14143102528236}\n",
      "Losses {'ner': 58.64978119290957}\n",
      "Losses {'ner': 59.61542419287183}\n",
      "Losses {'ner': 5.156708570686533}\n",
      "Losses {'ner': 6.1371947292991536}\n",
      "Losses {'ner': 8.733122264030044}\n",
      "Losses {'ner': 14.462640200736587}\n",
      "Losses {'ner': 17.667515047222253}\n",
      "Losses {'ner': 17.66777851262924}\n",
      "Losses {'ner': 21.6777764987699}\n",
      "Losses {'ner': 26.570575829598397}\n",
      "Losses {'ner': 26.576067400156774}\n",
      "Losses {'ner': 27.7995901321911}\n",
      "Losses {'ner': 32.15935433562032}\n",
      "Losses {'ner': 38.26044162268869}\n",
      "Losses {'ner': 41.94624800513617}\n",
      "Losses {'ner': 48.14386887381903}\n",
      "Losses {'ner': 3.2424211854949476}\n",
      "Losses {'ner': 8.288400264470454}\n",
      "Losses {'ner': 13.263571245203764}\n",
      "Losses {'ner': 14.232299825701812}\n",
      "Losses {'ner': 21.371514660224122}\n",
      "Losses {'ner': 26.70421922431494}\n",
      "Losses {'ner': 29.74084306360389}\n",
      "Losses {'ner': 33.79109530189241}\n",
      "Losses {'ner': 37.798172951098124}\n",
      "Losses {'ner': 41.899216789232625}\n",
      "Losses {'ner': 41.90218345606817}\n",
      "Losses {'ner': 48.86244343158441}\n",
      "Losses {'ner': 50.89734068404513}\n",
      "Losses {'ner': 54.75619224069678}\n",
      "Losses {'ner': 2.1971455740556927}\n",
      "Losses {'ner': 4.478342699511526}\n",
      "Losses {'ner': 7.951492997161662}\n",
      "Losses {'ner': 11.511611184130683}\n",
      "Losses {'ner': 13.9073819551873}\n",
      "Losses {'ner': 20.506168190042953}\n",
      "Losses {'ner': 23.446997029952225}\n",
      "Losses {'ner': 28.03657253917282}\n",
      "Losses {'ner': 35.01196434434479}\n",
      "Losses {'ner': 35.027493203506154}\n",
      "Losses {'ner': 39.94227919184398}\n",
      "Losses {'ner': 44.405252332720785}\n",
      "Losses {'ner': 50.74703594568732}\n",
      "Losses {'ner': 52.73403030876282}\n",
      "Losses {'ner': 3.8219370221922873}\n",
      "Losses {'ner': 7.7455554456864775}\n",
      "Losses {'ner': 11.416807472956862}\n",
      "Losses {'ner': 15.843827000138845}\n",
      "Losses {'ner': 23.191443434236135}\n",
      "Losses {'ner': 26.061906730425108}\n",
      "Losses {'ner': 29.98853286927988}\n",
      "Losses {'ner': 35.42734207969988}\n",
      "Losses {'ner': 36.38579829755136}\n",
      "Losses {'ner': 38.368733259345305}\n",
      "Losses {'ner': 40.307644845886365}\n",
      "Losses {'ner': 44.499967790241534}\n",
      "Losses {'ner': 47.35249879310678}\n",
      "Losses {'ner': 48.6206392972753}\n",
      "Losses {'ner': 4.617700889706612}\n",
      "Losses {'ner': 5.584194734198945}\n",
      "Losses {'ner': 7.564258348838166}\n",
      "Losses {'ner': 9.614795648202971}\n",
      "Losses {'ner': 14.922062316925121}\n",
      "Losses {'ner': 17.04647320095978}\n",
      "Losses {'ner': 24.114415894789317}\n",
      "Losses {'ner': 25.071588513761412}\n",
      "Losses {'ner': 28.32160208190873}\n",
      "Losses {'ner': 31.303604868334894}\n",
      "Losses {'ner': 37.10268143658288}\n",
      "Losses {'ner': 42.72586031652644}\n",
      "Losses {'ner': 44.52448779621954}\n",
      "Losses {'ner': 48.50349901013808}\n",
      "Losses {'ner': 5.234425083279348}\n",
      "Losses {'ner': 8.532865233188716}\n",
      "Losses {'ner': 15.878370702023517}\n",
      "Losses {'ner': 20.45728032266618}\n",
      "Losses {'ner': 25.634223028774272}\n",
      "Losses {'ner': 32.38688006600262}\n",
      "Losses {'ner': 37.194024977385766}\n",
      "Losses {'ner': 41.07950158963888}\n",
      "Losses {'ner': 47.35603135736403}\n",
      "Losses {'ner': 51.28507786850048}\n",
      "Losses {'ner': 54.44390421020387}\n",
      "Losses {'ner': 57.74968640958548}\n",
      "Losses {'ner': 60.63764619354998}\n",
      "Losses {'ner': 62.131621047623355}\n",
      "Losses {'ner': 4.5453350504431}\n",
      "Losses {'ner': 8.181836541616065}\n",
      "Losses {'ner': 13.376289496390598}\n",
      "Losses {'ner': 18.589321127028086}\n",
      "Losses {'ner': 23.075821895019033}\n",
      "Losses {'ner': 27.816169708306404}\n",
      "Losses {'ner': 27.82009364450886}\n",
      "Losses {'ner': 30.4958328592499}\n",
      "Losses {'ner': 30.685796100225787}\n",
      "Losses {'ner': 33.96646660314735}\n",
      "Losses {'ner': 37.38405147961174}\n",
      "Losses {'ner': 39.637048983060176}\n",
      "Losses {'ner': 42.15580204578831}\n",
      "Losses {'ner': 44.943556227177886}\n",
      "Losses {'ner': 3.9748932818274505}\n",
      "Losses {'ner': 4.934976171545693}\n",
      "Losses {'ner': 5.924332231395717}\n",
      "Losses {'ner': 8.89325842026269}\n",
      "Losses {'ner': 15.81787662213509}\n",
      "Losses {'ner': 18.434811325281487}\n",
      "Losses {'ner': 24.865498509858014}\n",
      "Losses {'ner': 30.405302591683338}\n",
      "Losses {'ner': 33.15871779777974}\n",
      "Losses {'ner': 39.36740354692376}\n",
      "Losses {'ner': 40.884215629615916}\n",
      "Losses {'ner': 41.86204679670348}\n",
      "Losses {'ner': 41.865986402098}\n",
      "Losses {'ner': 47.27773591182873}\n",
      "Losses {'ner': 3.6473435365342937}\n",
      "Losses {'ner': 9.681835982643946}\n",
      "Losses {'ner': 13.39705596340557}\n",
      "Losses {'ner': 19.75479637633746}\n",
      "Losses {'ner': 24.563481034576803}\n",
      "Losses {'ner': 31.289052994787603}\n",
      "Losses {'ner': 35.33600982552384}\n",
      "Losses {'ner': 37.60875412632817}\n",
      "Losses {'ner': 37.62557224693142}\n",
      "Losses {'ner': 42.28441067362569}\n",
      "Losses {'ner': 43.29908069503363}\n",
      "Losses {'ner': 43.30993413657911}\n",
      "Losses {'ner': 43.31087375383976}\n",
      "Losses {'ner': 46.99755445685622}\n"
     ]
    }
   ],
   "source": [
    "# Importing requirements\n",
    "from spacy.util import minibatch, compounding\n",
    "import random\n",
    "\n",
    "# Begin training by disabling other pipeline components\n",
    "with nlp.disable_pipes(*other_pipes) :\n",
    "    sizes = compounding(1.0, 4.0, 1.001)\n",
    "  # Training for 30 iterations     \n",
    "    for itn in range(30):\n",
    "        # shuffle examples before training\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        # batch up the examples using spaCy's minibatch\n",
    "        batches = minibatch(TRAIN_DATA, size=sizes)\n",
    "        # ictionary to store losses\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            # Calling update() over the iteration\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.35, losses=losses)\n",
    "            print(\"Losses\", losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training of our NER is complete now.\n",
    "\n",
    "Let’s test if the ner can identify our new entity. If it’s not upto your expectations, try include more training examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T10:18:57.700197Z",
     "start_time": "2020-07-23T10:18:57.682247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities in 'I ate Sushi yesterday. Maggi is a common fast food '\n",
      "Sushi\n",
      "Maggi\n"
     ]
    }
   ],
   "source": [
    "# Testing the NER\n",
    "\n",
    "test_text = \"I ate Sushi yesterday. Maggi is a common fast food \"\n",
    "doc = nlp(test_text)\n",
    "print(\"Entities in '%s'\" % test_text)\n",
    "for ent in doc.ents:\n",
    "    print(ent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe the above output. The model has correctly identified the FOOD items. Also, notice that I had not passed ” Maggi ” as a training example to the model. Still, based on the similarity of context, the model has identified “Maggi” also as FOOD. This is an important requirement! Our model should not just memorize the training examples. It should learn from them and generalize it to new examples.\n",
    "\n",
    "Once you find the performance of the model satisfactory , you can save the updated model to directory using to_disk command. You can load the model from the directory at any point of time by passing the directory path to spacy.load() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T10:19:58.121024Z",
     "start_time": "2020-07-23T10:19:57.424821Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to \\content\n",
      "Loading from \\content\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saurabhkumar9\\AppData\\Local\\Continuum\\anaconda2\\envs\\nlp\\lib\\site-packages\\spacy\\util.py:275: UserWarning: [W031] Model 'en_my_ner' (2.2.0) requires spaCy v2.2 and is incompatible with the current spaCy version (2.3.2). This may lead to unexpected results or runtime errors. To resolve this, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    }
   ],
   "source": [
    "# Output directory\n",
    "from pathlib import Path\n",
    "output_dir=Path('/content/')\n",
    "\n",
    "# Saving the model to the output directory\n",
    "if not output_dir.exists():\n",
    "    output_dir.mkdir()\n",
    "nlp.meta['name'] = 'my_ner'  # rename model\n",
    "nlp.to_disk(output_dir)\n",
    "print(\"Saved model to\", output_dir)\n",
    "\n",
    "# Loading the model from the directory\n",
    "print(\"Loading from\", output_dir)\n",
    "nlp2 = spacy.load(output_dir)\n",
    "assert nlp2.get_pipe(\"ner\").move_names == move_names\n",
    "doc2 = nlp2(' Dosa is an extremely famous south Indian dish')\n",
    "for ent in doc2.ents:\n",
    "    print(ent.label_, ent.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that the model works as per our expectations. This is how you can train a new additional entity type to the ‘Named Entity Recognizer’ of spaCy.\n",
    "\n",
    "I hope you have understood the when and how to use custom NERs. Custom Training of models has proven to be the gamechanger in many cases. It’s because of this flexibility, spaCy is widely used for NLP. Stay tuned for more such posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-23T13:33:15.177152Z",
     "start_time": "2020-07-23T13:33:15.152218Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-be6266336763>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[0mtest_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"I ate Sushi yesterday. Maggi is a common fast food\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[0mtest_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnlp2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-46-be6266336763>\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(model, examples)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mscorer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mScorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0minput_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;31m#print(input_)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mdoc_gold_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.scorer import Scorer\n",
    "\n",
    "def evaluate(model, examples):\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        #print(input_)\n",
    "        doc_gold_text = model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot['entities'])\n",
    "        pred_value = model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores\n",
    "\n",
    "test_data = \"I ate Sushi yesterday. Maggi is a common fast food\"\n",
    "\n",
    "test_result = evaluate(nlp2, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
