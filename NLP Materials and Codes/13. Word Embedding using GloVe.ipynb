{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GloVe stands for global vectors for word representation. It is an unsupervised learning algorithm developed by Stanford for generating word embeddings by aggregating global word-word co-occurrence matrix from a corpus. The resulting embeddings show interesting linear substructures of the word in vector space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Overview**\n",
    "\n",
    "The main insight of word2vec was that we can require semantic analogies to be preserved under basic arithmetic on the word vectors, e.g. king - man + woman = queen. (Really elegant and brilliant, if you ask me.) Mikolov, et al., achieved this through continuous bag of words and/or skipgram models (word2vec comes in both flavors) that are trained to be predictive of how native speakers would intuit analogies in the real world. Under this approach, all that matters is the distance between two words, and the intrinsic statistical properties of the corpus (which were key to most earlier methods) end up getting lost.\n",
    "\n",
    "Over at Stanford NLP, they liked the analogy preservation, but disliked losing transparency and direct consideration of the corpusâ€™ word occurrence statistics, which they regard as fundamental to the problem. They hypothesized that by identifying a way to achieve analogy preservation under linear arithmetic that uses only these fundamental statistical properties of the corpus as inputs, they would present an improvement in both accuracy and interpretability. The GloVe paper argues that they succeeded.\n",
    "\n",
    "**Differences b/w Word2Vec and GloVe:**\n",
    "\n",
    "* Presence of Neural Networks: GloVe does not use neural networks while word2vec does. In GloVe, the loss function is the difference between the product of word embeddings and the log of the probability of co-occurrence. We try to reduce that and use SGD but solve it as we would solve a linear regression. While in the case of word2vec, we either train the word on its context (skip-gram) or train the context on the word (continuous bag of words) using a 1-hidden layer neural network.\n",
    "* Global information: word2vec does not have any explicit global information embedded in it by default. GloVe creates a global co-occurrence matrix by estimating the probability a given word will co-occur with other words. This presence of global information makes GloVe ideally work better. Although in a practical sense, they work almost similar and people have found similar performance with both.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "spyder",
   "language": "python",
   "name": "spyder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
