{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Despite its great results on medium or large sized texts (>50 words), typically mails and news articles are about this size range, LDA poorly performs on short texts like Tweets, Reddit posts or StackOverflow titles’ questions.\n",
    "\n",
    "The Gibbs Sampling Dirichlet Mixture Model (GSDMM) is an “altered” LDA algorithm, showing great results on STTM tasks, that makes the initial assumption: 1 topic ↔️1 document. The words within a document are generated using the same unique topic, and not from a mixture of topics as it was in the original LDA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is GSDMM?\n",
    "\n",
    "Imagine a bunch of students in a restaurant, seating randomly at K tables. They are all asked to write their favorite movies on a paper (but it must remain a short list). The objective is to cluster them in such a way that so students within the same group share the same movie interest. To do so, one after another, students must make a new table choice regarding the two following rules:\n",
    "\n",
    "* Rule 1: Choose a table with more students. This rule improves completeness, all students sharing the same movie’s interest are assigned to the same table.\n",
    "* Rule 2: Choose a table where students share similar movie’s interest. This rule aims to increase homogeneity, we want only members sharing the same movie’s interest at a table.\n",
    "\n",
    "After repeating this process, we expect some tables to disappear and others to grow larger and eventually have clusters of students matching their movie’s interest. This is simply what the GSDMM algorithm does!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T10:56:50.118194Z",
     "start_time": "2020-07-24T10:56:49.037044Z"
    }
   },
   "outputs": [],
   "source": [
    "from preprocessing import tokenize, export_to_csv # a seperate class to be created\n",
    "from gsdmm import MovieGroupProcess\n",
    "from topic_allocation import top_words, topic_attribution\n",
    "from visualisation import plot_topic_notebook, save_topic_html\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "import pickle\n",
    "import matplotlib as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T10:57:10.520927Z",
     "start_time": "2020-07-24T10:57:08.668852Z"
    }
   },
   "outputs": [],
   "source": [
    "cats = ['talk.politics.mideast', 'comp.windows.x', 'sci.space']\n",
    "\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', remove=('headers', 'footers', 'quotes'), categories=cats)\n",
    "newsgroups_train_subject = fetch_20newsgroups(subset='train', categories=cats)\n",
    "\n",
    "data = newsgroups_train.data\n",
    "data_subject = newsgroups_train_subject.data\n",
    "\n",
    "targets = newsgroups_train.target.tolist()\n",
    "target_names = newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T10:57:25.622333Z",
     "start_time": "2020-07-24T10:57:25.597395Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    593\n",
       "0    593\n",
       "2    564\n",
       "Name: targets, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's see if our topics are evenly distributed\n",
    "df_targets = pd.DataFrame({'targets': targets})\n",
    "order_list = df_targets.targets.value_counts()\n",
    "order_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T10:57:47.163403Z",
     "start_time": "2020-07-24T10:57:47.117525Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_first_sentence(data_subject):\n",
    "    list_first_sentence = []\n",
    "    for text in data:\n",
    "        first_sentence = text.split(\".\")[0].replace(\"\\n\", \"\")\n",
    "        list_first_sentence.append(first_sentence)\n",
    "    return list_first_sentence\n",
    "\n",
    "\n",
    "def extract_subject(data):\n",
    "    c = 0\n",
    "    s = \"Subject:\"\n",
    "    list_subjects = []\n",
    "    for new in data_subject:    \n",
    "        lines = new.split(\"\\n\")\n",
    "        b = 0 # loop out at the first \"Subject:\", they may be several and we want first one only\n",
    "        for line in lines:\n",
    "            if s in line and b == 0:\n",
    "                subject = \" \".join(line.split(\":\")[1:]).strip()\n",
    "                subject = subject.replace('Re', '').strip()\n",
    "                list_subjects.append(subject)\n",
    "                c += 1\n",
    "                b = 1\n",
    "    return list_subjects\n",
    "   \n",
    "    \n",
    "def concatenate(list_first_sentence, list_subjects):\n",
    "    list_docs = []\n",
    "    for i in range(len(list_first_sentence)):\n",
    "        list_docs.append(list_subjects[i] + \" \" + list_first_sentence[i])\n",
    "    return list_docs\n",
    "\n",
    "\n",
    "list_first_sentence = extract_first_sentence(data)\n",
    "list_subjects = extract_subject(data_subject)\n",
    "list_docs = concatenate(list_first_sentence, list_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T10:58:02.616885Z",
     "start_time": "2020-07-24T10:58:02.579984Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>topic_id</th>\n",
       "      <th>topic_true_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Elevator to the top floor Reading from a Amoco...</td>\n",
       "      <td>1</td>\n",
       "      <td>space</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Title for XTerm Yet again,the escape sequences...</td>\n",
       "      <td>0</td>\n",
       "      <td>x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>From Israeli press. Madness. Before getting ex...</td>\n",
       "      <td>2</td>\n",
       "      <td>mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Accounts of Anti-Armenian Human Right Violatio...</td>\n",
       "      <td>2</td>\n",
       "      <td>mideast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How many israeli soldiers does it take to kill...</td>\n",
       "      <td>2</td>\n",
       "      <td>mideast</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             content  topic_id topic_true_name\n",
       "0  Elevator to the top floor Reading from a Amoco...         1           space\n",
       "1  Title for XTerm Yet again,the escape sequences...         0               x\n",
       "2  From Israeli press. Madness. Before getting ex...         2         mideast\n",
       "3  Accounts of Anti-Armenian Human Right Violatio...         2         mideast\n",
       "4  How many israeli soldiers does it take to kill...         2         mideast"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(columns=['content', 'topic_id', 'topic_true_name'])\n",
    "df['content'] = list_docs\n",
    "df['topic_id'] = targets\n",
    "\n",
    "def true_topic_name(x, target_names):\n",
    "    return target_names[x].split('.')[-1]\n",
    "\n",
    "df['topic_true_name'] = df['topic_id'].apply(lambda x: true_topic_name(x, target_names))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-24T10:58:25.310373Z",
     "start_time": "2020-07-24T10:58:25.216623Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-b01f1d37ed34>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtokenized_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mform_reduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'stemming'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'tokenize' is not defined"
     ]
    }
   ],
   "source": [
    "tokenized_data = tokenize(df, form_reduction='stemming', predict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "spyder",
   "language": "python",
   "name": "spyder"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
